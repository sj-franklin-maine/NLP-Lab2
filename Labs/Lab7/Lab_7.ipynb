{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj3L43iGZpGv"
      },
      "source": [
        "Libraries needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y26IMKsXC1mC",
        "outputId": "ae86e868-4142-45a2-bea3-e222af55eef2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('punkt')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEnAFoYc95aS"
      },
      "source": [
        "Preprocessing the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dDEdEuU2DXjc"
      },
      "outputs": [],
      "source": [
        "def remove_links_mentions(tweet):\n",
        "  # Preprocessing the tweets by removing links and mentions\n",
        "    link_re_pattern = \"https?:\\/\\/t.co/[\\w]+\"\n",
        "    mention_re_pattern = \"@\\w+\"\n",
        "    tweet = re.sub(link_re_pattern, \"\", tweet)\n",
        "    tweet = re.sub(mention_re_pattern, \"\", tweet)\n",
        "    return tweet.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2boBfrsi99xB"
      },
      "source": [
        "Getting train, validation, and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VwkojLqranb1"
      },
      "outputs": [],
      "source": [
        "def read_tweets_get_vectors(tweet_file):\n",
        "    df = pd.read_csv(tweet_file, sep=',', header=0)\n",
        "    dic_result = {}\n",
        "    df1 = df[['tweet_id', 'text', 'airline_sentiment']]\n",
        "    count = 0\n",
        "    for index in range(len(df1)):\n",
        "        try:\n",
        "            label = df.loc[index, \"airline_sentiment\"]\n",
        "            tweet_id = df.loc[index, \"tweet_id\"]\n",
        "            if label in dic_result:\n",
        "                dic_result[label][tweet_id] = remove_links_mentions(df.loc[index, \"text\"])\n",
        "            else:\n",
        "                dic_result[label] = {tweet_id: remove_links_mentions(df.loc[index, \"text\"])}\n",
        "        except:\n",
        "            count += 1\n",
        "    return dic_result\n",
        "\n",
        "\n",
        "def split_data(twitter_data):\n",
        "    training = []\n",
        "    validation = []\n",
        "    test = []\n",
        "\n",
        "    for label in twitter_data:\n",
        "        temp_dic = twitter_data[label]\n",
        "        lst_tweet_ids = list(temp_dic.keys())\n",
        "        train_length = int(len(lst_tweet_ids) * 0.8)\n",
        "        train_ids = lst_tweet_ids[:train_length]\n",
        "        remaining = lst_tweet_ids[train_length:]\n",
        "        test_lenght = int(len(remaining) * 0.5)\n",
        "        test_ids = remaining[:test_lenght]\n",
        "        validation_id = remaining[test_lenght:]\n",
        "\n",
        "        for tweet_id in train_ids:\n",
        "            training.append((label, temp_dic[tweet_id]))\n",
        "        for tweet_id in validation_id:\n",
        "            validation.append((label, temp_dic[tweet_id]))\n",
        "        for tweet_id in test_ids:\n",
        "            test.append((label, temp_dic[tweet_id]))\n",
        "\n",
        "    random.shuffle(training)\n",
        "    random.shuffle(validation)\n",
        "    random.shuffle(test)\n",
        "    return training, validation, test\n",
        "\n",
        "\n",
        "dic_tweets = read_tweets_get_vectors(\"Tweets.csv\")\n",
        "training, validation, test = split_data(dic_tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y_kK2bW-9lJ"
      },
      "source": [
        "Building Vocabulary of the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0FuRiFV_B-5",
        "outputId": "7fdb5cbc-81d6-45f2-8a95-5b62eedb3e9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24669"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index2word = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
        "\n",
        "for ds in [training, validation, test]:\n",
        "    for label, tweet in ds:\n",
        "        for token in tweet.split(\" \"):\n",
        "            if token not in index2word:\n",
        "                index2word.append(token)\n",
        "\n",
        "word2index = {token: idx for idx, token in enumerate(index2word)}\n",
        "len(word2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3NZWrYGVDYTr"
      },
      "outputs": [],
      "source": [
        "def label_map(label):\n",
        "    if label == \"negative\":\n",
        "        return [1, 0, 0]\n",
        "    elif label == \"neutral\":\n",
        "        return [0, 1, 0]\n",
        "    else:  # positive\n",
        "        return [0, 0, 1]\n",
        "\n",
        "##################\n",
        "# Seq Length is an important parameter to tune\n",
        "##################\n",
        "\n",
        "seq_length = 20\n",
        "\n",
        "def encode_and_pad(tweet, length):\n",
        "    sos = [word2index[\"<SOS>\"]]\n",
        "    eos = [word2index[\"<EOS>\"]]\n",
        "    pad = [word2index[\"<PAD>\"]]\n",
        "    tweet_terms = tweet.split(\" \")\n",
        "    if len(tweet_terms) < length - 2:  # -2 for SOS and EOS\n",
        "        n_pads = length - 2 - len(tweet_terms)\n",
        "        encoded = [word2index[w] for w in tweet_terms]\n",
        "        return sos + encoded + eos + pad * n_pads\n",
        "    else:  # tweet is longer than possible; truncating\n",
        "        encoded = [word2index[w] for w in tweet_terms]\n",
        "        truncated = encoded[:length - 2]\n",
        "        return sos + truncated + eos\n",
        "\n",
        "\n",
        "train_encoded = [(encode_and_pad(tweet, seq_length), label_map(label)) for label, tweet in training]\n",
        "validation_encoded = [(encode_and_pad(tweet, seq_length), label_map(label)) for label, tweet in validation]\n",
        "test_encoded = [(encode_and_pad(tweet, seq_length), label_map(label)) for label, tweet in test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "xHB-EH_i_rBw"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "\n",
        "train_x = np.array([tweet for tweet, label in train_encoded])\n",
        "train_y = np.array([label for tweet, label in train_encoded])\n",
        "validation_x = np.array([tweet for tweet, label in validation_encoded])\n",
        "validation_y = np.array([label for tweet, label in validation_encoded])\n",
        "test_x = np.array([tweet for tweet, label in test_encoded])\n",
        "test_y = np.array([label for tweet, label in test_encoded])\n",
        "\n",
        "train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "validation_ds = TensorDataset(torch.from_numpy(validation_x), torch.from_numpy(validation_y))\n",
        "test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "validation_dl = DataLoader(validation_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_dl = DataLoader(test_ds, shuffle=True, batch_size=batch_size, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "m44Dr_NNFZ8z"
      },
      "outputs": [],
      "source": [
        "class RNN_SentimentAnalysis(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, num_layers, hidden_dim, bidirectional, output, dropout) :\n",
        "        super().__init__()\n",
        "\n",
        "        # The embedding layer takes the vocab size and the embeddings size as input\n",
        "        # The embeddings size is up to you to decide, but common sizes are between 50 and 100.\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        if not bidirectional:\n",
        "          self.num_directions = 1\n",
        "        else:\n",
        "          self.num_directions = 2\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_dim\n",
        "        # The RNN layer takes in the the embedding size and the hidden vector size.\n",
        "        # The hidden dimension is up to you to decide, but common values are 32, 64, 128\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "        # We use dropout before the final layer to improve with regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # The fully-connected layer takes in the hidden dim of the RNN and\n",
        "        #  outputs a a 3x1 vector of the class scores.\n",
        "        self.fc = nn.Linear(hidden_dim*self.num_directions, output)\n",
        "        self.tanh=nn.Tanh()\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        The forward method takes in the input and the previous hidden state \n",
        "        \"\"\"\n",
        "\n",
        "        # The input is transformed to embeddings by passing it to the embedding layer\n",
        "        embs = self.embedding(x)\n",
        "\n",
        "        # The embedded inputs are fed to the RNN alongside the previous hidden state\n",
        "        out, hidden = self.rnn(embs, hidden)\n",
        "\n",
        "        # Dropout is applied to the output and fed to the FC layer\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        out = self.tanh(out)\n",
        "        # We extract the scores for the final hidden state since it is the one that matters.\n",
        "        out = out[:, -1]\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.ones(self.num_layers*self.num_directions, batch_size, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_N5RaIGFcei"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(word2index)\n",
        "embedding_dim = 100 \n",
        "num_layers = 2\n",
        "hidden_dim = 256\n",
        "output = 3\n",
        "dropout = 0.5\n",
        "bidirectional = False\n",
        "model = RNN_SentimentAnalysis(len(word2index), embedding_dim, num_layers, hidden_dim, bidirectional, output, dropout)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "\n",
        "epochs = 60\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val = 100\n",
        "best_model = None\n",
        "for e in range(epochs):\n",
        "\n",
        "    h0 =  model.init_hidden(batch_size)\n",
        "\n",
        "    h0 = h0.to(device)\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dl):\n",
        "\n",
        "        input = batch[0].to(device)\n",
        "        target = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            out, hidden = model(input, h0)\n",
        "            # out = out.reshape(-1)\n",
        "            target = torch.squeeze(target.float())\n",
        "            loss = criterion(out, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    for batch_idx, batch in enumerate(validation_dl):\n",
        "        input = batch[0].to(device)\n",
        "        target = batch[1].to(device)\n",
        "        with torch.set_grad_enabled(False):\n",
        "            out, hidden = model(input, h0)\n",
        "            _, preds = torch.max(out, 1)\n",
        "            # out = out.reshape(-1)\n",
        "            target = torch.squeeze(target.float())\n",
        "            loss = criterion(out, target)\n",
        "    val_loss = loss.item()\n",
        "    val_losses.append(val_loss)\n",
        "    if val_loss<best_val:\n",
        "      best_val = val_loss\n",
        "      best_model = model\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)\n",
        "plt.title('First Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l923rrOHfzA",
        "outputId": "5fd5ecf2-0b9f-4924-a376-967750da80f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6787109375\n"
          ]
        }
      ],
      "source": [
        "batch_acc = []\n",
        "for batch_idx, batch in enumerate(test_dl):\n",
        "    input = batch[0].to(device)\n",
        "    target = batch[1].to(device)\n",
        "    with torch.set_grad_enabled(False):\n",
        "        out, hidden = best_model(input, h0)\n",
        "        target = target.argmax (dim = 1)\n",
        "        preds = out.argmax(dim=1)\n",
        "        batch_acc.append(accuracy_score(preds, target))\n",
        "\n",
        "print(sum(batch_acc) / len(batch_acc))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
