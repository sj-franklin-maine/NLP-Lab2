{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reading Law Stack Exchange Data"
      ],
      "metadata": {
        "id": "1vC6-KZifEAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from post_parser_record import PostParserRecord\n",
        "from gensim.models import FastText\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def read_tsv_test_data(file_path):\n",
        "  # Takes in the file path for test file and generate a dictionary\n",
        "  # of question id as the key and the list of question ids similar to it\n",
        "  # as value. It also returns the list of all question ids that have\n",
        "  # at least one similar question\n",
        "  dic_similar_questions = {}\n",
        "  lst_all_test = []\n",
        "  with open(file_path) as fd:\n",
        "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
        "    for row in rd:\n",
        "        question_id = int(row[0])\n",
        "        lst_similar = list(map(int, row[1:]))\n",
        "        dic_similar_questions[question_id] = lst_similar\n",
        "        lst_all_test.append(question_id)\n",
        "        lst_all_test.extend(lst_similar)\n",
        "  return dic_similar_questions, lst_all_test\n",
        "\n",
        "\n",
        "def train_model(lst_sentences):\n",
        "  #model = None\n",
        "  model = FastText(\n",
        "        sentences=lst_sentences,\n",
        "        vector_size=100,    # size of the word vectors\n",
        "        window=5,    # window size for the skip-gram model\n",
        "        min_count=5, # minimum count of words to include in the vocabulary\n",
        "        sg=1,        # use skip-gram model\n",
        "        workers=4    # number of worker threads to use\n",
        "    )\n",
        "  model.build_vocab(corpus_iterable=lst_sentences)\n",
        "  # train the model\n",
        "  model.train(\n",
        "      corpus_iterable=lst_sentences,\n",
        "      total_examples=len(lst_sentences),\n",
        "      epochs=10\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "M7PNV1oxfBbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e7355f-36db-422d-fb8a-010389a4db3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# separating out to save time\n",
        "duplicate_file = \"duplicate_questions.tsv\"\n",
        "post_file = \"Posts_law.xml\"\n",
        "dic_similar_questions, lst_all_test = read_tsv_test_data(duplicate_file)\n",
        "#print(dic_similar_questions)\n",
        "post_reader = PostParserRecord(post_file)\n",
        "lst_training_sentences = []\n",
        "embeddings = {}\n",
        "for question_id in post_reader.map_questions:\n",
        "  if question_id in lst_all_test:\n",
        "    continue\n",
        "  question = post_reader.map_questions[question_id]\n",
        "  title = question.title\n",
        "  body = question.body\n",
        "  # Collect sentences here\n",
        "  processed_title = re.sub('<[^<]+?>', ' ', title)\n",
        "  token_title = nltk.sent_tokenize(processed_title)\n",
        "  processed_body = re.sub('<[^<]+?>', ' ', title)\n",
        "  token_body = nltk.sent_tokenize(processed_body)\n",
        "\n",
        "  lst_training_sentences.extend(token_title)\n",
        "  lst_training_sentences.extend(token_body)"
      ],
      "metadata": {
        "id": "EiKkQnVTu8Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train your model\n",
        "model = train_model(lst_training_sentences)\n",
        "\n",
        "# save model\n",
        "model.save(\"fastText.model\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IfM8HNUvESi",
        "outputId": "a1c8e348-4d4e-4767-9f3f-7e690df686ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.keyedvectors:sorting after vectors have been allocated is expensive & error-prone\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  # get embeddings for each sentence and average them\n",
        "  title_embedding = np.zeros(100)\n",
        "  body_embedding = np.zeros(100)\n",
        "  for sentence in token_title:\n",
        "      title_embedding += get_sentence_embedding(model, sentence)\n",
        "  title_embedding /= len(token_title)\n",
        "  for sentence in token_body:\n",
        "      body_embedding += get_sentence_embedding(model, sentence)\n",
        "  body_embedding /= len(token_body)\n",
        "\n",
        "  embeddings[question_id] = np.concatenate((title_embedding, body_embedding))\n",
        "\n",
        "  lst_answers = question.answers\n",
        "  if lst_answers is not None:\n",
        "    for answer in lst_answers:\n",
        "      answer_body = answer.body\n",
        "      # Collection sentences here\n",
        "      answer_body = re.sub('<[^<]+?>', ' ', answer_body)\n",
        "      # Tokenize the answer body\n",
        "      answer_sents = nltk.word_tokenize(answer_body)\n",
        "      # Add each sentence to the list\n",
        "      lst_training_sentences.extend(answer_sents)\n",
        "      \n",
        "# use your model and calculate the cosine similarity between the questions\n",
        "# save the question id with the highest cosine similarity\n",
        "# finding Similar questions using fastText model\n",
        "  for test_question_id in dic_similar_questions:\n",
        "    test_question_embedding = embeddings[test_question_id]\n",
        "    for similar_question_id in dic_similar_questions[test_question_id]:\n",
        "      similar_question_embedding = embeddings[similar_question_id]\n",
        "      similarity = 1 - spatial.distance.cosine(test_question_embedding, similar_question_embedding)\n",
        "      print(\"Cosine Similarity between question\", test_question_id, \"and similar question\", similar_question_id, \":\", similarity)\n",
        "          \n",
        "  # finding Similar questions using fastText model\n",
        "  total_p_1 = 0.0\n",
        "  for test_question_id in dic_similar_questions:\n",
        "    test_question = dic_similar_questions[test_question_id]['Question']\n",
        "    expected_duplicate_id = dic_similar_questions[test_question_id]['DuplicateId']\n",
        "    predicted_duplicate_id = model.wv.most_similar(test_question, topn=1)[0][0]\n",
        "    if predicted_duplicate_id == expected_duplicate_id:\n",
        "        total_p_1 += 1.0\n",
        "    else:\n",
        "        total_p_1 += 0.0\n",
        "    dictionary_result[test_question_id] = predicted_duplicate_id\n",
        "\n",
        "  # calculate average P@1\n",
        "  num_test_questions = len(dic_similar_questions)\n",
        "  avg_p_1 = total_p_1 / num_test_questions\n",
        "  print(\"Average P@1: {:.4f}\".format(avg_p_1))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "BKDTkUqhuWoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: FFNN"
      ],
      "metadata": {
        "id": "UtKAvYhKsblp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the neural network architecture\n",
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Define the dataset\n",
        "class QuestionPairDataset(Dataset):\n",
        "    def __init__(self, pairs, labels):\n",
        "        self.pairs = pairs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question1 = self.pairs[idx][0]\n",
        "        question2 = self.pairs[idx][1]\n",
        "        label = self.labels[idx]\n",
        "        return {'question1': question1, 'question2': question2, 'label': label}"
      ],
      "metadata": {
        "id": "ETPBJDmAsgoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "train_size = int(0.8 * len(pairs))\n",
        "val_size = int(0.1 * len(pairs))\n",
        "test_size = len(pairs) - train_size - val_size\n",
        "\n",
        "train_pairs = pairs[:train_size]\n",
        "train_labels = labels[:train_size]\n",
        "\n",
        "val_pairs = pairs[train_size:train_size+val_size]\n",
        "val_labels = labels[train_size:train_size+val_size]\n",
        "\n",
        "test_pairs = pairs[train_size+val_size:]\n",
        "test_labels = labels[train_size+val_size:]\n",
        "\n",
        "# Convert data into PyTorch DataLoader\n",
        "train_dataset = QuestionPairDataset(train_pairs, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "val_dataset = QuestionPairDataset(val_pairs, val_labels)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "test_dataset = QuestionPairDataset(test_pairs, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Initialize the neural network\n",
        "input_size = 768  # Embedding size of the input questions\n",
        "hidden_size = 64\n",
        "model = FeedForwardNN(input_size*2, hidden_size)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    for batch in train_dataloader\n",
        "    # Get the inputs and labels\n",
        "    question1_embeddings = batch['question1']\n",
        "    question2_embeddings = batch['question2']\n",
        "    labels = batch['label'].float()\n",
        "    # Zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    inputs = torch.cat((question1_embeddings, question2_embeddings), dim=1)\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs.squeeze(), labels)\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Accumulate loss\n",
        "    train_loss += loss.item() * len(labels)\n",
        "\n",
        "# Compute validation loss\n",
        "val_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        # Get the inputs and labels\n",
        "        question1_embeddings = batch['question1']\n",
        "        question2_embeddings = batch['question2']\n",
        "        labels = batch['label'].float()\n",
        "\n",
        "        # Forward pass\n",
        "        inputs = torch.cat((question1_embeddings, question2_embeddings), dim=1)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "\n",
        "        # Accumulate loss\n",
        "        val_loss += loss.item() * len(labels)\n",
        "\n",
        "# Compute average loss for the epoch\n",
        "train_loss /= len(train_dataset)\n",
        "val_loss /= len(val_dataset)\n",
        "\n",
        "# Print progress\n",
        "print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "test_loss = 0.0\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "for batch in test_dataloader:\n",
        "# Get the inputs and labels\n",
        "question1_embeddings = batch['question1']\n",
        "question2_embeddings = batch['question2']\n",
        "labels = batch['label'].float()\n",
        "\n",
        "    # Forward pass\n",
        "    inputs = torch.cat((question1_embeddings, question2_embeddings), dim=1)\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs.squeeze(), labels)\n",
        "\n",
        "    # Accumulate loss and predictions\n",
        "    test_loss += loss.item() * len(labels)\n",
        "    predictions.extend(outputs.squeeze().tolist())\n",
        "\n",
        "    test_loss /= len(test_dataset)\n",
        "    predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
        "    correct = sum([1 if p == l else 0 for p, l in zip(predictions, test_labels)])\n",
        "    accuracy = correct / len(test_labels)\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "SKrfsYJ8FZr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}