{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Example of training fastText model and getting sentence embeddings"
      ],
      "metadata": {
        "id": "xl0K2GQZe8kb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HCFRB215b45H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8578402-d7dc-4413-b6c9-4182676796a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7853913903236389\n",
            "0.8745558857917786\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import FastText\n",
        "from scipy import spatial\n",
        "\n",
        "\n",
        "def get_sentence_embedding(model, sentence):\n",
        "  # This method takes in the trained model and the input sentence\n",
        "  # and returns the embedding of the sentence as the average embedding\n",
        "  # of its words\n",
        "  words = sentence.split(\" \")\n",
        "  vector = model.wv[words[0]]\n",
        "  for i in range(1, len(words)):\n",
        "    vector += model.wv[words[i]]\n",
        "  return vector/len(words)\n",
        "\n",
        "\n",
        "sampleTexts = [\"This is example1\", \"This is example two\", \"This is example three\"]\n",
        "# There are parameters here that you should define\n",
        "model = FastText(vector_size = 100, window = 5, min_n=1)\n",
        "model.build_vocab(sampleTexts)\n",
        "\n",
        "# training the model\n",
        "model.train(sampleTexts, total_examples = len(sampleTexts), epochs = 10)\n",
        "\n",
        "# saving the model in-case you need to reuse it\n",
        "model.save(\"fastText.model\")\n",
        "\n",
        "vec1 = get_sentence_embedding(model, sampleTexts[0])\n",
        "vec2 = get_sentence_embedding(model, sampleTexts[1])\n",
        "vec3 = get_sentence_embedding(model, sampleTexts[2])\n",
        "\n",
        "# calculating cosine similarity\n",
        "result = 1 - spatial.distance.cosine(vec1, vec2)\n",
        "print(result)\n",
        "\n",
        "result = 1 - spatial.distance.cosine(vec1, vec3)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Law Stack Exchange Data"
      ],
      "metadata": {
        "id": "1vC6-KZifEAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from post_parser_record import PostParserRecord\n",
        "from gensim.models import FastText\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def read_tsv_test_data(file_path):\n",
        "  # Takes in the file path for test file and generate a dictionary\n",
        "  # of question id as the key and the list of question ids similar to it\n",
        "  # as value. It also returns the list of all question ids that have\n",
        "  # at least one similar question\n",
        "  dic_similar_questions = {}\n",
        "  lst_all_test = []\n",
        "  with open(file_path) as fd:\n",
        "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
        "    for row in rd:\n",
        "        question_id = int(row[0])\n",
        "        lst_similar = list(map(int, row[1:]))\n",
        "        dic_similar_questions[question_id] = lst_similar\n",
        "        lst_all_test.append(question_id)\n",
        "        lst_all_test.extend(lst_similar)\n",
        "  return dic_similar_questions, lst_all_test\n",
        "\n",
        "\n",
        "def train_model(lst_sentences):\n",
        "  #model = None\n",
        "  model = FastText(\n",
        "        sentences=lst_sentences,\n",
        "        vector_size=100,    # size of the word vectors\n",
        "        window=5,    # window size for the skip-gram model\n",
        "        min_count=5, # minimum count of words to include in the vocabulary\n",
        "        sg=1,        # use skip-gram model\n",
        "        workers=4    # number of worker threads to use\n",
        "    )\n",
        "  model.build_vocab(corpus_iterable=lst_sentences)\n",
        "  # train the model\n",
        "  model.train(\n",
        "      corpus_iterable=lst_sentences,\n",
        "      total_examples=len(lst_sentences),\n",
        "      epochs=10\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "M7PNV1oxfBbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e7355f-36db-422d-fb8a-010389a4db3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# separating out to save time\n",
        "duplicate_file = \"duplicate_questions.tsv\"\n",
        "post_file = \"Posts_law.xml\"\n",
        "dic_similar_questions, lst_all_test = read_tsv_test_data(duplicate_file)\n",
        "#print(dic_similar_questions)\n",
        "post_reader = PostParserRecord(post_file)\n",
        "lst_training_sentences = []\n",
        "embeddings = {}\n",
        "for question_id in post_reader.map_questions:\n",
        "  if question_id in lst_all_test:\n",
        "    continue\n",
        "  question = post_reader.map_questions[question_id]\n",
        "  title = question.title\n",
        "  body = question.body\n",
        "  # Collect sentences here\n",
        "  processed_title = re.sub('<[^<]+?>', ' ', title)\n",
        "  token_title = nltk.sent_tokenize(processed_title)\n",
        "  processed_body = re.sub('<[^<]+?>', ' ', title)\n",
        "  token_body = nltk.sent_tokenize(processed_body)\n",
        "\n",
        "  lst_training_sentences.extend(token_title)\n",
        "  lst_training_sentences.extend(token_body)"
      ],
      "metadata": {
        "id": "EiKkQnVTu8Gm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train your model\n",
        "model = train_model(lst_training_sentences)\n",
        "\n",
        "# save model\n",
        "model.save(\"fastText.model\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IfM8HNUvESi",
        "outputId": "a1c8e348-4d4e-4767-9f3f-7e690df686ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.keyedvectors:sorting after vectors have been allocated is expensive & error-prone\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  # get embeddings for each sentence and average them\n",
        "  title_embedding = np.zeros(100)\n",
        "  body_embedding = np.zeros(100)\n",
        "  for sentence in token_title:\n",
        "      title_embedding += get_sentence_embedding(model, sentence)\n",
        "  title_embedding /= len(token_title)\n",
        "  for sentence in token_body:\n",
        "      body_embedding += get_sentence_embedding(model, sentence)\n",
        "  body_embedding /= len(token_body)\n",
        "\n",
        "  embeddings[question_id] = np.concatenate((title_embedding, body_embedding))\n",
        "\n",
        "  lst_answers = question.answers\n",
        "  if lst_answers is not None:\n",
        "    for answer in lst_answers:\n",
        "      answer_body = answer.body\n",
        "      # Collection sentences here\n",
        "      answer_body = re.sub('<[^<]+?>', ' ', answer_body)\n",
        "      # Tokenize the answer body\n",
        "      answer_sents = nltk.word_tokenize(answer_body)\n",
        "      # Add each sentence to the list\n",
        "      lst_training_sentences.extend(answer_sents)\n",
        "      \n",
        "# use your model and calculate the cosine similarity between the questions\n",
        "# save the question id with the highest cosine similarity\n",
        "# finding Similar questions using fastText model\n",
        "  for test_question_id in dic_similar_questions:\n",
        "    test_question_embedding = embeddings[test_question_id]\n",
        "    for similar_question_id in dic_similar_questions[test_question_id]:\n",
        "      similar_question_embedding = embeddings[similar_question_id]\n",
        "      similarity = 1 - spatial.distance.cosine(test_question_embedding, similar_question_embedding)\n",
        "      print(\"Cosine Similarity between question\", test_question_id, \"and similar question\", similar_question_id, \":\", similarity)\n",
        "          \n",
        "  # finding Similar questions using fastText model\n",
        "  total_p_1 = 0.0\n",
        "  for test_question_id in dic_similar_questions:\n",
        "    test_question = dic_similar_questions[test_question_id]['Question']\n",
        "    expected_duplicate_id = dic_similar_questions[test_question_id]['DuplicateId']\n",
        "    predicted_duplicate_id = model.wv.most_similar(test_question, topn=1)[0][0]\n",
        "    if predicted_duplicate_id == expected_duplicate_id:\n",
        "        total_p_1 += 1.0\n",
        "    else:\n",
        "        total_p_1 += 0.0\n",
        "    dictionary_result[test_question_id] = predicted_duplicate_id\n",
        "\n",
        "  # calculate average P@1\n",
        "  num_test_questions = len(dic_similar_questions)\n",
        "  avg_p_1 = total_p_1 / num_test_questions\n",
        "  print(\"Average P@1: {:.4f}\".format(avg_p_1))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "BKDTkUqhuWoi",
        "outputId": "369eef6f-428d-4f3d-c059-c9ca8b9771fc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d9e4429968b2>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average P@1: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_p_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-d9e4429968b2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# finding Similar questions using fastText model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtest_question_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic_similar_questions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtest_question_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_question_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msimilar_question_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic_similar_questions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_question_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0msimilar_question_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msimilar_question_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3526"
          ]
        }
      ]
    }
  ]
}